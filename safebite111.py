# -*- coding: utf-8 -*-
"""Copy of SafeBite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wj-p87XCr3jm8sNGJTou0p4O5u1S2da8
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import os
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.models import Model
from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, concatenate
from keras.utils import to_categorical
from keras.losses import BinaryCrossentropy

data = pd.read_csv('Gabungan1.csv')

# Pisahkan fitur (Ingredients) dan label (Potensial Allergies, Potensial Diseases, Halal/Haram)
X = data['Ingredients']
y_allergies = data['Potential Allergies']
y_diseases = data['Potential Diseases']
y_halal = data['Halal/Haram']

#Check the features and labels
print(data.columns)

#Check the class name and the number of the class (Potential Allergies)
print('class_name:', data['Potential Allergies'].unique())
print('the number of class:', len(data['Potential Allergies'].unique()))

#Check the class name and the number of the class (Potential Diseases)
print('class_name:', data['Potential Diseases'].unique())
print('the number of class:', len(data['Potential Diseases'].unique()))

#Check the class name and the number of the class (Halal/Haram)
print('class_name:', data['Halal/Haram'].unique())
print('the number of class:', len(data['Halal/Haram'].unique()))

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Preprocessing teks
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_sequences = tokenizer.texts_to_sequences(X)
X_padded = pad_sequences(X_sequences)
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
max_length = X_padded.shape[1]

# Melakukan encoding pada label Potensial Allergies
label_encoder_allergies = LabelEncoder()
y_encoded_allergies = label_encoder_allergies.fit_transform(y_allergies)

# Melakukan encoding pada label Potensial Diseases
label_encoder_diseases = LabelEncoder()
y_encoded_diseases = label_encoder_diseases.fit_transform(y_diseases)

# Melakukan encoding pada label Halal/Haram
label_encoder_halal = LabelEncoder()
y_encoded_halal = label_encoder_halal.fit_transform(y_halal)

# Memisahkan data menjadi subset training (80%) dan test (20%)
X_train, X_test, y_train_allergies, y_test_allergies, y_train_diseases, y_test_diseases, y_train_halal, y_test_halal = train_test_split(
    X_padded, y_encoded_allergies, y_encoded_diseases, y_encoded_halal, test_size=0.2, random_state=42
)

# Memisahkan subset training menjadi subset training dan validation
X_train, X_val, y_train_allergies, y_val_allergies, y_train_diseases, y_val_diseases, y_train_halal, y_val_halal = train_test_split(
    X_train, y_train_allergies, y_train_diseases, y_train_halal, test_size=0.2, random_state=42
)

# Mengecek pembagian subset training, validation, testing
print("Jumlah data total:", len(X_padded))
print("Jumlah data training:", len(X_train))
print("Jumlah data validation:", len(X_val))
print("Jumlah data test:", len(X_test))

# Menentukan jumlah kelas alergi dalam setiap label yang telah diencoding
num_classes_allergies = len(np.unique(y_encoded_allergies))
y_train_allergies_encoded = to_categorical(y_train_allergies, num_classes=num_classes_allergies)
y_test_allergies_encoded = to_categorical(y_test_allergies, num_classes=num_classes_allergies)

# Menentukan jumlah kelas penyakit dalam setiap label yang telah diencoding
num_classes_diseases = len(np.unique(y_encoded_diseases))
y_train_diseases_encoded = to_categorical(y_train_diseases, num_classes=num_classes_diseases)
y_test_diseases_encoded = to_categorical(y_test_diseases, num_classes=num_classes_diseases)

# Menentukan jumlah kelas halal/haram dalam setiap label yang telah diencoding
num_classes_halal = len(np.unique(y_encoded_halal))
y_train_halal_encoded = to_categorical(y_train_halal, num_classes=num_classes_halal)
y_test_halal_encoded = to_categorical(y_test_halal, num_classes=num_classes_halal)

# Arsitektur model untuk klasifikasi potensial alergi
input_allergies = Input(shape=(max_length,))
embedding_allergies = Embedding(vocab_size, embedding_dim)(input_allergies)
conv_allergies = Conv1D(256, 3, activation='relu')(embedding_allergies)
pooling_allergies = GlobalMaxPooling1D()(conv_allergies)
dropout_allergies = Dropout(0.5)(pooling_allergies)
hidden_allergies = Dense(32, activation='relu')(dropout_allergies)
output_allergies = Dense(num_classes_allergies, activation='softmax')(hidden_allergies)
model_allergies = Model(inputs=input_allergies, outputs=output_allergies)

# Arsitektur model untuk klasifikasi potensial penyakit
input_diseases = Input(shape=(max_length,))
embedding_diseases = Embedding(vocab_size, embedding_dim)(input_diseases)
conv_diseases = Conv1D(256, 3, activation='relu')(embedding_diseases)
pooling_diseases = GlobalMaxPooling1D()(conv_diseases)
dropout_diseases = Dropout(0.5)(pooling_diseases)
hidden_diseases = Dense(32, activation='relu')(dropout_diseases)
output_diseases = Dense(num_classes_diseases, activation='softmax')(hidden_diseases)
model_diseases = Model(inputs=input_diseases, outputs=output_diseases)

# Arsitektur model untuk klasifikasi halal/haram
input_halal = Input(shape=(max_length,))
embedding_halal = Embedding(vocab_size, embedding_dim)(input_halal)
conv_halal = Conv1D(128, 5, activation='relu')(embedding_halal)
pooling_halal = GlobalMaxPooling1D()(conv_halal)
dropout_halal = Dropout(0.5)(pooling_halal)
hidden_halal = Dense(64, activation='relu')(dropout_halal)
output_halal = Dense(num_classes_halal, activation='softmax')(hidden_halal)
model_halal = Model(inputs=input_halal, outputs=output_halal)

model_allergies.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_diseases.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_halal.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history_allergies = model_allergies.fit(X_train, y_train_allergies, validation_data=(X_val, y_val_allergies), epochs=150, batch_size=64, shuffle=True, verbose=2)

model_allergies.save("allergies.h5")

history_diseases = model_diseases.fit(X_train, y_train_diseases, validation_data=(X_val, y_val_diseases), epochs=100, batch_size=32, shuffle=True, verbose=2)

model_diseases.save("diseases.h5")

history_halal = model_halal.fit(X_train, y_train_halal, validation_data=(X_val, y_val_halal), epochs=30, batch_size=32, shuffle=True, verbose=2)

model_halal.save("halal.h5")

# Menggabungkan output dari ketiga model
concatenated = concatenate([output_allergies, output_diseases, output_halal])
ensemble_output = Dense(3, activation='softmax')(concatenated)

# Menggabungkan ketiga model menjadi 1 model class dari Keras
ensemble_model = Model(inputs=[input_allergies, input_diseases, input_halal], outputs=ensemble_output)

loss_allergies, accuracy_allergies = model_allergies.evaluate(X_test, y_test_allergies)
print("Potensial Allergies - Loss: {:.4f} - Accuracy: {:.4f}".format(loss_allergies, accuracy_allergies))

loss_diseases, accuracy_diseases = model_diseases.evaluate(X_test, y_test_diseases)
print("Potensial Diseases - Loss: {:.4f} - Accuracy: {:.4f}".format(loss_diseases, accuracy_diseases))

loss_halal, accuracy_halal = model_halal.evaluate(X_test, y_test_halal)
print("Halal/Haram - Loss: {:.4f} - Accuracy: {:.4f}".format(loss_halal, accuracy_halal))

model_allergies = keras.models.load_model("allergies.h5")
model_diseases = keras.models.load_model("diseases.h5")
model_halal = keras.models.load_model("halal.h5")

# Misalkan X_new_data adalah data baru yang ingin Anda prediksi
data_uji = pd.read_csv('Gabungan1.csv')
X_new_data = data_uji['Ingredients']

# Lakukan preprocessing pada data baru (misalnya, tokenisasi dan pengkodean)
X_new_sequences = tokenizer.texts_to_sequences(X_new_data)
X_new_padded = pad_sequences(X_new_sequences, maxlen=max_length)

# Lakukan prediksi untuk masing-masing model
predictions_allergies = model_allergies.predict(X_new_padded)
predictions_diseases = model_diseases.predict(X_new_padded)
predictions_halal = model_halal.predict(X_new_padded)

# Menggabungkan prediksi dari ketiga model
concatenated_predictions = np.concatenate([predictions_allergies, predictions_diseases, predictions_halal], axis=1)

# Menentukan label prediksi berdasarkan nilai probabilitas tertinggi
label_prediksi_allergies = label_encoder_allergies.inverse_transform(np.argmax(predictions_allergies, axis=1))
label_prediksi_diseases = label_encoder_diseases.inverse_transform(np.argmax(predictions_diseases, axis=1))
label_prediksi_halal = label_encoder_halal.inverse_transform(np.argmax(predictions_halal, axis=1))

# Tampilkan hasil prediksi
print(concatenated_predictions)

# Menampilkan hasil prediksi
for i in range(len(X_new_data)):
    print('Data:', X_new_data[i])
    print('Label Prediksi Allergy:', label_prediksi_allergies[i])
    print('Label Prediksi Diseases:', label_prediksi_diseases[i]) 
    print('Label Prediksi Halal / Haram:', label_prediksi_halal[i])   
    print('------------------------')

from flask import Flask, request, jsonify

model_allergies = keras.models.load_model("allergies.h5")
model_diseases = keras.models.load_model("diseases.h5")
model_halal = keras.models.load_model("halal.h5")

# Fungsi untuk membaca teks dari hasil OCR
def read_ocr_text(filename): # apa yg seharusnya di panggil
    return predict (filename)
  #  with open(filename, 'r') as file: 
  #      ocrText = file.read() 
   # return ocrText

def predict (x):
  prediction_allergies = model_allergies.predict(x)
  prediction_diseases = model_diseases.predict(x)
  prediction_halal = model_halal.predict(x)
  
  label_prediksi_allergies = np.argmax(prediction_allergies[0])
  label_prediksi_diseases = np.argmax(prediction_diseases[0])
  label_prediksi_halal = np.argmax(prediction_halal[0])
  
  return label_prediksi_allergies, label_prediksi_diseases, label_prediksi_halal

app = Flask(__name__)

@app.route('/', methods=['POST'])
def index():
  if request.method == 'POST':
    try:
      input_data = request.get_json()
      if not input_data:
        return jsonify({"error": "no data"})
 
      feature = input_data.get('feature')
      #return feature
    
      # Membaca teks dari hasil OCR
      ocrText = read_ocr_text(feature) #error
    
      # Preprocessing teks
      tokenizer = Tokenizer()
      tokenizer.fit_on_texts(ocrText)
      input_sequence = tokenizer.texts_to_sequences([ocrText])
      max_sequence_length = 7
      input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)
      
      # Memprediksi menggunakan model
      prediction_allergies, prediction_diseases, prediction_halal = predict(input_padded)
      data = {
          "prediction_allergies": prediction_allergies,
          "prediction_diseases": prediction_diseases,
          "prediction_halal": prediction_halal
          }
      return jsonify(data)
    except Exception as e:
      return jsonify({"error": str(e)})

  return "OK"

if __name__ == '__main__':
  app.run(debug = True)